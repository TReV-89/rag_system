# RAG System

## Project Description

The **RAG System** (Retrieval-Augmented Generation System) is an application designed to enable question answering over a large text document using modern LLMs (Large Language Models) and retrieval techniques. The main purpose is to provide a conversational interface, powered by Streamlit, allowing users to ask questions about a specific document and receive grounded, accurate answers supported by content from the document itself.

For this project, the chosen default document is **"War and Peace" by Leo Tolstoy**, included as `rag_document/war and peace.txt`. Users can also upload their own `.txt` or `.pdf` files for question answering.

---

## Application Architecture

The system is built on the following components:

- **Streamlit**: Serves as the user interface, handling chat, file upload, and document interaction.
- **Langchain**: Handles document loading, chunking, and communication with LLMs.
- **ChromaDB**: Vector database used for storing and retrieving document embeddings.
- **Google Generative AI**: Provides both the embedding model (via Gemini API) and the LLM for generating answers.

**Architecture Flow:**
1. User interacts via Streamlit app.
2. On startup, the app loads the default document (or uploaded document), splits it into chunks, generates embeddings, and stores these in ChromaDB.
3. When a user asks a question, relevant chunks are retrieved from ChromaDB, and the LLM answers using only the retrieved chunks as context.

---

## Chunking Strategy and Embedding Model

- **Chunking**: The document is split using Langchain's `RecursiveCharacterTextSplitter` with a chunk size of **800 characters** and an overlap of **50 characters**. This balances context for the LLM while avoiding memory overload.
- **Embedding Model**: The system uses the **GoogleGenerativeAiEmbeddingFunction** from ChromaDB, which leverages Gemini API for embedding generation.
- **LLM**: Answers are generated by the Gemini model via the `langchain-google-genai` integration.

---

## Step-by-Step Instructions

### 1. Clone the Repository

```bash
git clone https://github.com/TReV-89/rag_system.git
cd rag_system/front_end
```

### 2. Set Up Environment Variables

Create a `.env` file in the `front_end` directory with the following keys:

```
GEMINI_API_KEY=your_google_gemini_api_key
GEMINI_MODEL=your preferred Gemini model
```

> **Important:** Never commit your `.env` file or API keys to version control.

### 3. Build the Docker Image

Make sure Docker is installed. Build the Docker Compose container:

```bash
docker compose up -- build
```


### 4. Access the Streamlit Application

Once the stack is running, open your browser and go to:

```
http://localhost:8501
```

You should see the RAG System interface.

---

### 5. Assumptions Made

- The default document is "War and Peace" (`rag_document/war and peace.txt`).
- Only `.txt` and `.pdf` files are supported for upload.
- Chunk size and overlap are tuned for "War and Peace" but can be adjusted in `rag_methods.py`.
- The environment requires access to Gemini API keys for both embedding and LLM.
- ChromaDB runs as a separate service and is accessible at `chroma:8000` (Docker Compose networking).
- The deployment assumes a Linux VPS with Docker and Docker Compose installed.

---

## 6. Public URL

The deployed application is accessible at:

```

```

---

## Contact

For questions or support, please open an issue in this repository.
